bad_test <- test % 2 = 0
bad_test <- test / 2 = 2
bad_test <- test > 2
test[bad_test]
test[-bad_test]
test[-(bad_test)]
test[test not in bad_test]
bad_test
!bad_test
good_predictions <- predict(mlr_model, strat_data[!bad_indices, ]$setId)
good_predictions <- predict(mlr_model, good_data$setId)
good_data <- strat_data[!bad_indices]
good_predictions <- predict(mlr_model, good_data$setId)
good_data
good_predictions <- predict(mlr_model, good_data$setId)
good_predictions <- predict(mlr_model, good_data)
max(good_predictions)
min(good_predictions)
mean(good_predictions)
sd(good_predictions)
good_predictions_rmse <- rmse(good_predictions, good_data$setId)
good_predictions_rmse
good_metrics_mae <- mae(good_predictions, good_data$setId)
good_metrics_mae
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./merged.csv")
vec <- vector()
vec[1] <- c(1,2,3)
vec <- vector(vector())
vec <- vector()
vec[1] <- vector()
vec[1] <- c(1,2,3)
#Separate out each benmark into vector of vectors
pgbench <- wholeSet$pgbench
sysbench <- wholeSet$sysbench
ycruncher <- wholeSet$ycruncher
iperf <- wholeSet$iperf
sds <- data.frame(vector(), vector(), vector(), vector())
colnames(sds) <- c("pgbench", "sysbench", "ycruncher", "iperf")
head(sds)
for (i in 1:48) {
set <- wholeSet[wholeSet$setId == i]
a <- sd(set$pgbench)
b <- sd(set$sysbench)
c <- sd(set$ycruncher)
d <- sd(set$iperf)
sds <- rbind(sds, cbind(a, b, c, d))
}
for (i in 1:48) {
set <- wholeSet[wholeSet$setId == i, ]
a <- sd(set$pgbench)
b <- sd(set$sysbench)
c <- sd(set$ycruncher)
d <- sd(set$iperf)
sds <- rbind(sds, cbind(a, b, c, d))
}
View(sds)
View(wholeSet)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./merged.csv")
#Separate out each benmark into vector of vectors
pgbench <- wholeSet$pgbench
sysbench <- wholeSet$sysbench
ycruncher <- wholeSet$ycruncher
iperf <- wholeSet$iperf * 1000
sds <- data.frame(vector(), vector(), vector(), vector())
benchmarks <- c("pgbench", "sysbench", "ycruncher", "iperf")
colnames(sds) <- benchmarks
head(sds)
for (i in 1:48) {
set <- wholeSet[wholeSet$setId == i, ]
a <- sd(set$pgbench)
b <- sd(set$sysbench)
c <- sd(set$ycruncher)
d <- sd(set$iperf)
sds <- rbind(sds, cbind(a, b, c, d))
}
sd(wholeSet$pgbench[wholeSet$setId == 1])
sd(wholeSet$pgbench[wholeSet$setId == 2])
sd(wholeSet$sysbench[wholeSet$setId == 1])
sd(wholeSet$sysbench[wholeSet$setId == 2])
sd(wholeSet$ycruncher[wholeSet$setId == 1])
sd(wholeSet$ycruncher[wholeSet$setId == 2])
sd(wholeSet$iperf[wholeSet$setId == 1])
sd(wholeSet$iperf[wholeSet$setId == 2])
View(wholeSet)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
library(randomForest)
library(rpart)
set.seed(100)
wholeSet = read.csv("./merged.csv")
#Convert to mbs, only if they are in gbs
wholeSet$iperf <- wholeSet$iperf * 1000
str(wholeSet)
# Loading the dplyr package
library(dplyr)
formula = setId ~ iperf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(formula, data=wholeSet, ntree=2000, na.action=na.exclude, importance=TRUE)
print(modelRandomForest)
saveRDS(modelRandomForest, "./modelRandomForest.rds")
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
#Clear workspace
rm(list=ls())
library(tidyverse)
library(splitstackshape)
#load DedicatedHost Data
set.seed(100)
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
iperf_scaled <- scale(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
## Get mean and sd for each benchmark.  Use this to scale the prediction data
means <- vector()
sds <- vector()
means[1] <- mean(wholeSet$iperf)
sds[1] <- mean(wholeSet$iperf)
means[1] <- mean(wholeSet$iperf)
sds[1] <- sd(wholeSet$iperf)
sds
View(wholeSet)
means[1] <- mean(wholeSet$iperf)
sds[1] <- sd(wholeSet$iperf)
means[1] <- mean(iperf_scaled)
iperf_scaled
mean(iperf_scaled)
sd(iperf_scaled)
means[1] <- mean(pgbench_scaled)
sds[1] <- sd(pgbench$iperf)
sds[1] <- sd(pgbench_scaled)
means[2] <- mean(ycruncher_scaled)
sds[2] <- sd(ycruncher_scaled)
means[3] <- mean(sysbench_scaled)
sds[3] <- sd(sysbench_scaled)
means
sds
means[4] <- mean(iperf_scaled, na.rm=TRUE)
sds[4] <- sd(iperf_scaled, na.rm=TRUE)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
View(wholeSet)
View(wholeSet)
## Get mean and sd for each benchmark.  Use this to scale the prediction data
means <- vector()
sds <- vector()
means[1] <- mean(sysbench_scaled)
sds[1] <- sd(sysbench_scaled)
means[2] <- mean(ycruncher_scaled)
sds[2] <- sd(ycruncher_scaled)
means[3] <- mean(pgbench_scaled)
sds[3] <- sd(pgbench_scaled)
means[4] <- mean(iperf_scaled, na.rm=TRUE)
sds[4] <- sd(iperf_scaled, na.rm=TRUE)
save(means, "./means.rds")
save(means, file="./means.rds")
save(sds, file="./sds.rds")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(mlr_model)
iperf_scaled <- scale(wholeSet$iperf)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(mlr_model)
means <- load("./means.rds")
sds <- load("/sds.rds")
sds <- load("./sds.rds")
sds
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(mlr_model)
means <- load("./means.rds")
means <- load("means.rds")
means <- load("means.rds")
means <- readRds("./means.rds")
means <- readRds("./means.rds")
means <- readRDS("./means.rds")
#Clear workspace
rm(list=ls())
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
means <- vector()
sds <- vector()
scale(wholeSet[, "iperf", "sysbench", "pgbench", "ycruncher"])
iperf_scaled <- scale(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
#Clear workspace
rm(list=ls())
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
iperf_scaled <- scale(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
means <- vector()
sds <- vector()
means[1] <- mean(sysbench_scaled)
sds[1] <- sd(sysbench_scaled)
means[2] <- mean(ycruncher_scaled)
sds[2] <- sd(ycruncher_scaled)
means[3] <- mean(pgbench_scaled)
sds[3] <- sd(pgbench_scaled)
means[4] <- mean(iperf_scaled, na.rm=TRUE)
sds[4] <- sd(iperf_scaled, na.rm=TRUE)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(mlr_model)
View(wholeSet)
View(wholeSet)
sets <- wholeSet$setId
wholeSet[1:5, 2:5]
newSet <- wholeSet[, 2:5]
View(newSet)
newScaleSet <- scale(newSet, center=means, scale=sds)
View(newScaleSet)
View(newScaleSet)
iperf_scaled <- (wholeSet$iperf - mean(wholeSet$instanceID)) / sd(wholeSet$instanceID)
iperf_scaled <- (wholeSet$iperf - mean(wholeSet$iperf)) / sd(wholeSet$instanceID)
iperf_scaled <- (wholeSet$iperf - mean(wholeSet$iperf)) / sd(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- (wholeSet$pgbench - mean(wholeSet$pgbench)) / sd(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- (wholeSet$sysbench - mean(wholeSet$sysbench)) / sd(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- (wholeSet$ycruncher - mean(wholeSet$ycruncher)) / sd(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
means <- vector()
sds <- vector()
means[1] <- mean(sysbench_scaled)
sds[1] <- sd(sysbench_scaled)
means[2] <- mean(ycruncher_scaled)
sds[2] <- sd(ycruncher_scaled)
means[3] <- mean(pgbench_scaled)
sds[3] <- sd(pgbench_scaled)
means[4] <- mean(iperf_scaled, na.rm=TRUE)
sds[4] <- sd(iperf_scaled, na.rm=TRUE)
model <- lm(setId ~ pgbench + sysbench + ycruncher, data = wholeSet)
summary(model)
model <- lm(setId ~ iperf + pgbench + sysbench + ycruncher, data = wholeSet)
summary(model)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
iperf_scaled <- (wholeSet$iperf - mean(wholeSet$iperf)) / sd(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- (wholeSet$pgbench - mean(wholeSet$pgbench)) / sd(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- (wholeSet$sysbench - mean(wholeSet$sysbench)) / sd(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- (wholeSet$ycruncher - mean(wholeSet$ycruncher)) / sd(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
means <- vector()
sds <- vector()
means[1] <- mean(sysbench_scaled)
sds[1] <- sd(sysbench_scaled)
means[2] <- mean(ycruncher_scaled)
sds[2] <- sd(ycruncher_scaled)
means[3] <- mean(pgbench_scaled)
sds[3] <- sd(pgbench_scaled)
means[4] <- mean(iperf_scaled, na.rm=TRUE)
sds[4] <- sd(iperf_scaled, na.rm=TRUE)
#Clear workspace
rm(list=ls())
set.seed(100)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
iperf_scaled <- (wholeSet$iperf - mean(wholeSet$iperf)) / sd(wholeSet$iperf)
#Clear workspace
rm(list=ls())
set.seed(100)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
iperf_scaled <- (wholeSet$iperf - mean(wholeSet$iperf,C)) / sd(wholeSet$iperf, na.rm=TRUE)
iperf_scaled <- (wholeSet$iperf - mean(wholeSet$iperf, na.rm=TRUE)) / sd(wholeSet$iperf, na.rm=TRUE)
summary(iperf_scaled)
pgbench_scaled <- (wholeSet$pgbench - mean(wholeSet$pgbench)) / sd(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- (wholeSet$sysbench - mean(wholeSet$sysbench)) / sd(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- (wholeSet$ycruncher - mean(wholeSet$ycruncher)) / sd(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
means <- vector()
sds <- vector()
means[1] <- mean(sysbench_scaled)
sds[1] <- sd(sysbench_scaled)
means[2] <- mean(ycruncher_scaled)
sds[2] <- sd(ycruncher_scaled)
means[3] <- mean(pgbench_scaled)
sds[3] <- sd(pgbench_scaled)
means[4] <- mean(iperf_scaled, na.rm=TRUE)
sds[4] <- sd(iperf_scaled, na.rm=TRUE)
View(wholeSet)
model <- lm(setId ~ iperf + pgbench + sysbench + ycruncher, data = wholeSet)
summary(model)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
summary(model)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
# load the model
#mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(model)
sets <- wholeSet$setId
newSet <- wholeSet[, 2:5]
View(newSet)
#This doesn't seem to make any difference
newSet$iperf <- (newSet$iperf - means[4]) / sds[4]
iperf_scaled[1:5]
#This doesn't seem to make any difference
newSet$iperf <- (newSet$iperf - means[4]) / sds[4]
newSet$sysbench <- (newSet$sysbench - means[1]) / sds[1]
newSet$ycruncher <- (newSet$ycruncher - means[2]) / sds[2]
newSet$pgbench <- (newSet$pgbench - means[3]) / sds[3]
predictions <- predict(mlr_model, newSet)
predictions <- predict(model, newSet)
predictions
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
summary(model)
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
#calculate the rmse
library("ModelMetrics")
predictions_rmse <- rmse(predictions, wholeSet$setId)
predictions_rmse
metrics_mae <- mae(predictions, wholeSet$setId)
metrics_mae
#Clear workspace
rm(list=ls())
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(mlr_model)
summary (mlr_model) # model summary
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
#Clear workspace
rm(list=ls())
library(tidyverse)
library(splitstackshape)
#load DedicatedHost Data
set.seed(100)
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
iperf_scaled <- scale(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
model <- lm(setId ~ iperf + pgbench + sysbench + ycruncher, data = wholeSet)
summary(model)
#Save this model
saveRDS(model, "./model_mlr_unstrat.rds")
#Clear workspace
rm(list=ls())
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(mlr_model)
iperf_scaled <- scale(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
predictions <- predict(model, wholeSet)
predictions
predictions <- predict(mlr_modell, wholeSet)
predictions <- predict(mlr_model, wholeSet)
predictions
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
#calculate the rmse
library("ModelMetrics")
predictions_rmse <- rmse(predictions, wholeSet$setId)
predictions_rmse
metrics_mae <- mae(predictions, wholeSet$setId)
metrics_mae
#Calculate the absolute error for each data point/prediction
abs_error <- vector()
plot(predictions, wholeSet$setId, xlab="Predicted # of VMs", ylab="Actual # of VMs", col="blue", main="MLR")
abline(a=0,b=1)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
library("randomForest")
#load Open Cloud data
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
# load the model
super_model <- readRDS("modelRandomForest.rds")
# load the model
super_model <- readRDS("modelRandomForest.rds")
print(super_model)
summary(super_model)
predictions <- predict(super_model, wholeSet)
predictions
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
predictions_rmse <- rmse(predictions, wholeSet$setId)
predictions_rmse
metrics_mae <- mae(predictions, wholeSet$setId)
metrics_mae
plot(predictions, wholeSet$setId, xlab="Predicted # of VMs", ylab="Actual # of VMs", col="blue", main="Untreated Random Forest")
abline(a=0,b=1)
plot(predictions, wholeSet$setId, xlab="Predicted # of VMs", ylab="Actual # of VMs", col="blue", main="Random Forest")
abline(a=0,b=1)
