result <- iperf[(iperf$setId == setid & iperf$vmId == vmid), ][1:3, ]
result <- na.omit(result)
new_iperf <- rbind(new_iperf, result)
}
}
View(new_sysbench)
View(new_sysbench)
new_sysbench$setId <- new_sysbench$setId * -1 + 48
new_sysbench <- new_sysbench[order(new_sysbench$setId), ]
#Notice that as of now the vmids are not in the right order.
#Should start with 1 and go consecutively, will fix later.
print(head(new_sysbench))
#This data set looks good already
print(head(new_iperf))
View(new_ycruncher)
new_ycruncher$setId <- new_ycruncher$setId * -1 + 48
new_ycruncher <- new_ycruncher[order(new_ycruncher$setId), ]
print(head(new_ycruncher))
View(pgbench)
pgbench$setId <- pgbench$setId * -1 + 48
pgbench <- pgbench[order(pgbench$setId), ]
print(head(pgbench))
#Now fix the vmids
pgbench$vmId <- new_iperf$vmId
new_sysbench$vmId <- new_iperf$vmId
new_ycruncher$vmId <- new_iperf$vmId
View(new_sysbench)
View(new_ycruncher)
#All data sets should have same dimensions
print(dim(new_iperf))
print(dim(pgbench))
print(dim(new_sysbench))
print(dim(new_ycruncher))
#Put all data into a dataframe
globalMerged = data.frame()
for (set in 1:48) {
for (vm in 1:set) {
a = new_iperf[new_iperf$setId == set & new_iperf$vmId == vm,]
b = new_sysbench[new_sysbench$setId == set & new_sysbench$vmId == vm,]
c = new_ycruncher[new_ycruncher$setId == set & new_ycruncher$vmId == vm,]
d = pgbench[pgbench$setId == set & pgbench$vmId == vm,]
merged = cbind(a["i_perf"],b["sysbench"],c["yCruncher"],d["pgbench"],rep.int(set, 3), rep.int(vm, 3))
globalMerged = rbind(globalMerged, merged)
}
}
View(globalMerged)
colnames(globalMerged)[colnames(globalMerged)=="rep.int(set, 3)"] <- set
colnames(globalMerged)[colnames(globalMerged)=="rep.int(set, 3)"] <- "set"
colnames(globalMerged)[colnames(globalMerged)=="48"] <- "set"
colnames(globalMerged)[colnames(globalMerged)=="rep.int(vm, 3)"] <- "vm"
#write it out as merged.csv.
write.csv(globalMerged,"./merged.csv",row.names=FALSE, col.names=FALSE)
#Clear workspace
rm(list=ls())
library(randomForest)
library(rpart)
wholeSet = read.csv("./merged.csv")
str(wholeSet)
# Loading the dplyr package
library(dplyr)
# Using sample_frac to create 70 - 30 slipt into test and train
train <- sample_frac(wholeSet, 0.9)
sample_id <- as.numeric(rownames(train)) # rownames() returns character so as.numeric
test <- wholeSet[-sample_id,]
formula = set~i_perf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(
formula,
data=train)
print(modelRandomForest)
modelRandomForest <- randomForest(
formula,
data=train)
#Clear workspace
rm(list=ls())
wholeSet = read.csv("./merged.csv")
library(randomForest)
library(rpart)
str(wholeSet)
# Loading the dplyr package
library(dplyr)
# Using sample_frac to create 70 - 30 slipt into test and train
train <- sample_frac(wholeSet, 0.9)
sample_id <- as.numeric(rownames(train)) # rownames() returns character so as.numeric
test <- wholeSet[-sample_id,]
formula = set~iperf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(
formula,
data=train)
print(modelRandomForest)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
library(randomForest)
library(rpart)
set.seed(100)
wholeSet = read.csv("./merged.csv")
str(wholeSet)
# Loading the dplyr package
library(dplyr)
formula = setId~iperf+sysbench+ycruncher+pgbench
modelRandomForest <- randomForest(formula, data=wholeSet, ntree=2000, na.action=na.exclude)
# Importance of each predictor.
print(importance(modelRandomForest,type = 2))
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
#Clear workspace
rm(list=ls())
library(tidyverse)
library(splitstackshape)
#load DedicatedHost Data
set.seed(100)
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
#Get rid of iperf data
wholeSet$iperf <- NULL
head(wwholeSet)
head(wholeSet)
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
pgbench_scaled <- scale(strat_data$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(strat_data$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(strat_data$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
strat_data$iperf <- iperf_scaled
strat_data$pgbench <- pgbench_scaled
strat_data$sysbench <- sysbench_scaled
strat_data$ycruncher <- ycruncher_scaled
model <- lm(setId ~ pgbench + sysbench + ycruncher, data = strat_data)
summary(model)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
#Clear workspace
rm(list=ls())
library(tidyverse)
library(splitstackshape)
#load DedicatedHost Data
set.seed(100)
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
iperf_scaled <- scale(strat_data $iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(strat_data$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(strat_data$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(strat_data$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
strat_data$iperf <- iperf_scaled
strat_data$pgbench <- pgbench_scaled
strat_data$sysbench <- sysbench_scaled
strat_data$ycruncher <- ycruncher_scaled
#Lets leave out iperf for now
model <- lm(setId ~ pgbench + sysbench + ycruncher, data = strat_data)
summary(model)
#Save this model
saveRDS(model, "./model_mlr_treated_no_iperf.rds")
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library("ModelMetrics")
library(nnet)
library(splitstackshape)
# load the model
mlr_model <- readRDS("./model_mlr_treated_no_iperf.rds")
print(mlr_model)
summary (mlr_model) # model summary
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
#Scale data
#Scale data to make it normally distributed
#iperf_log <- log(strat_data$iperf)
#summary(iperf_log)
iperf_scaled <- scale(strat_data$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(strat_data$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(strat_data$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(strat_data$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
strat_data$iperf <- iperf_scaled
strat_data$pgbench <- pgbench_scaled
strat_data$sysbench <- sysbench_scaled
strat_data$ycruncher <- ycruncher_scaled
predictions <- predict(mlr_model, strat_data)
predictions
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
predictions_rmse <- rmse(predictions, strat_data$setId)
predictions_rmse
metrics_mae <- mae(predictions, strat_data$setId)
metrics_mae
#Calculate the absolute error for each data point/prediction
abs_error <- vector()
for (i in 1:length(predictions)) {
abs_error[i] <- abs(predictions[i] - strat_data$setId[i])
}
#This should give us the mean of the abs error for predictions that should be 48 vms.
mean(abs_error[1:3])
sum(abs_error[1:3]) / 3
write.csv(abs_error, "./treated_mlr_abs_error_no_iperf.csv")
plot(predictions, strat_data$setId, xlab="Predicted # of VMs", ylab="Actual # of VMs", col="blue", main="Treated MLR")
abline(a=0,b=1)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/")
#Clear workspace
rm(list=ls())
library(tidyverse)
library(splitstackshape)
#load DedicatedHost Data
set.seed(100)
wholeSet = read.csv("./merged.csv")
wholeSet$iperf <- wholeSet$iperf * 1000
iperf_scaled <- scale(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
model <- lm(setId ~ pgbench + sysbench + ycruncher, data = wholeSet)
summary(model)
#Save this model
saveRDS(model, "./model_mlr_unstrat.rds")
#Save this model
saveRDS(model, "./model_mlr_unstrat_no_iperf.rds")
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat_no_iperf.rds")
print(mlr_model)
summary (mlr_model) # model summary
iperf_scaled <- scale(wholeSet$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(wholeSet$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(wholeSet$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(wholeSet$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
wholeSet$iperf <- iperf_scaled
wholeSet$pgbench <- pgbench_scaled
wholeSet$sysbench <- sysbench_scaled
wholeSet$ycruncher <- ycruncher_scaled
predictions <- predict(mlr_model, wholeSet)
predictions
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
#calculate the rmse
library("ModelMetrics")
predictions_rmse <- rmse(predictions, wholeSet$setId)
predictions_rmse
metrics_mae <- mae(predictions, wholeSet$setId)
metrics_mae
#Calculate the absolute error for each data point/prediction
abs_error <- vector()
for (i in 1:length(predictions)) {
abs_error[i] <- abs(predictions[i] - wholeSet$setId[i])
}
#This should give us the mean of the abs error for predictions that should be 48 vms.
mean(abs_error[1:48])
write.csv(abs_error, "./unstratified_abs_error.csv")
plot(predictions, wholeSet$setId, xlab="Predicted # of VMs", ylab="Actual # of VMs", col="blue", main="Unstratified MLR")
abline(a=0,b=1)
install.packages("sjmisc")
#Calculate r^2
library("sjmisc")
summary(mlr_model)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library("ModelMetrics")
library(nnet)
library(splitstackshape)
# load the model
mlr_model <- readRDS("./model_mlr_treated_no_iperf.rds")
print(mlr_model)
summary (mlr_model) # model summary
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library("ModelMetrics")
library(nnet)
library(splitstackshape)
# load the model
mlr_model <- readRDS("./model_mlr_treated.rds")
print(mlr_model)
summary (mlr_model) # model summary
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library(nnet)
# load the model
mlr_model <- readRDS("./model_mlr_unstrat.rds")
print(mlr_model)
summary (mlr_model) # model summary
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
library("randomForest")
#load Open Cloud data
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
# load the model
super_model <- readRDS("modelRandomForest.rds")
print(super_model)
summary(super_model)
print(super_model)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
library("randomForest")
#load Open Cloud data
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
# load the model
super_model <- readRDS("modelRandomForest_Treated.rds")
print(super_model)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d/DedicatedHost/")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./Aggregate_Summary_Dedicated_Host_11-16-2019.csv")
library("ModelMetrics")
library(nnet)
library(splitstackshape)
# load the model
mlr_model <- readRDS("./model_mlr_treated.rds")
print(mlr_model)
summary (mlr_model) # model summary
#Statify a new dataset so that we have same number of samples for each vm tenancy.
strat_data <- stratified(indt=wholeSet, group=c("setId"), size=3)
#Scale data
#Scale data to make it normally distributed
#iperf_log <- log(strat_data$iperf)
#summary(iperf_log)
iperf_scaled <- scale(strat_data$iperf)
summary(iperf_scaled)
pgbench_scaled <- scale(strat_data$pgbench)
summary(pgbench_scaled)
sysbench_scaled <- scale(strat_data$sysbench)
summary(sysbench_scaled)
ycruncher_scaled <- scale(strat_data$ycruncher)
summary(ycruncher_scaled)
#Lets use the scaled data to train the model
strat_data$iperf <- iperf_scaled
strat_data$pgbench <- pgbench_scaled
strat_data$sysbench <- sysbench_scaled
strat_data$ycruncher <- ycruncher_scaled
predictions <- predict(mlr_model, strat_data)
predictions
max(predictions)
min(predictions)
mean(predictions)
sd(predictions)
predictions_rmse <- rmse(predictions, strat_data$setId)
predictions_rmse
metrics_mae <- mae(predictions, strat_data$setId)
metrics_mae
#Match bad predictions with instance ids and get predictors
predictions[predictions > 2]
#Calculate the absolute error for each data point/prediction
abs_error <- vector()
for (i in 1:length(predictions)) {
abs_error[i] <- abs(predictions[i] - strat_data$setId[i])
}
#Match bad predictions with instance ids and get predictors
abs_err[abs_err > 2]
#Match bad predictions with instance ids and get predictors
abs_error[abs_error > 2]
#Match bad predictions with instance ids and get predictors
abs_error[abs_error > 3]
#Match bad predictions with instance ids and get predictors
abs_error[abs_error > 4]
#Match bad predictions with instance ids and get predictors
abs_error > 4
#Match bad predictions with instance ids and get predictors
bad_indices <- abs_error > 4
strat_data[bad_indices,]
## Lets look at the raw values.
strat_data1 <- stratified(indt=wholeSet, group=c("setId"), size=3)
strat_data1[bad_indices,]
good_predictions <- predict(mlr_model, strat_data1[-bad_indices, ]$setId)
strat_data1$iperf <- scale(strat_data$iperf)
strat_data1$pgbench <- scale(strat_data$pgbench)
strat_data1$sysbench <- scale(strat_data$sysbench)
strat_data1$ycruncher <- scale(strat_data$ycruncher)
good_predictions <- predict(mlr_model, strat_data1[-bad_indices, ]$setId)
good_predictions <- predict(mlr_model, strat_data[-bad_indices, ]$setId)
good_predictions <- predict(mlr_model, strat_data[-bad_indices, ])
test <- c(1, 2, 3, 4, 5, 6, 7, 8)
test
bad_test <- test % 2 == 0
bad_test <- test % 2 = 0
bad_test <- test / 2 = 2
bad_test <- test > 2
test[bad_test]
test[-bad_test]
test[-(bad_test)]
test[test not in bad_test]
bad_test
!bad_test
good_predictions <- predict(mlr_model, strat_data[!bad_indices, ]$setId)
good_predictions <- predict(mlr_model, good_data$setId)
good_data <- strat_data[!bad_indices]
good_predictions <- predict(mlr_model, good_data$setId)
good_data
good_predictions <- predict(mlr_model, good_data$setId)
good_predictions <- predict(mlr_model, good_data)
max(good_predictions)
min(good_predictions)
mean(good_predictions)
sd(good_predictions)
good_predictions_rmse <- rmse(good_predictions, good_data$setId)
good_predictions_rmse
good_metrics_mae <- mae(good_predictions, good_data$setId)
good_metrics_mae
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./merged.csv")
vec <- vector()
vec[1] <- c(1,2,3)
vec <- vector(vector())
vec <- vector()
vec[1] <- vector()
vec[1] <- c(1,2,3)
#Separate out each benmark into vector of vectors
pgbench <- wholeSet$pgbench
sysbench <- wholeSet$sysbench
ycruncher <- wholeSet$ycruncher
iperf <- wholeSet$iperf
sds <- data.frame(vector(), vector(), vector(), vector())
colnames(sds) <- c("pgbench", "sysbench", "ycruncher", "iperf")
head(sds)
for (i in 1:48) {
set <- wholeSet[wholeSet$setId == i]
a <- sd(set$pgbench)
b <- sd(set$sysbench)
c <- sd(set$ycruncher)
d <- sd(set$iperf)
sds <- rbind(sds, cbind(a, b, c, d))
}
for (i in 1:48) {
set <- wholeSet[wholeSet$setId == i, ]
a <- sd(set$pgbench)
b <- sd(set$sysbench)
c <- sd(set$ycruncher)
d <- sd(set$iperf)
sds <- rbind(sds, cbind(a, b, c, d))
}
View(sds)
View(wholeSet)
setwd("/home/ravschoo/ResourceContention/IaaSCloudResourceContention/modeldata/m5d")
#Clear workspace
rm(list=ls())
set.seed(100)
wholeSet = read.csv("./merged.csv")
#Separate out each benmark into vector of vectors
pgbench <- wholeSet$pgbench
sysbench <- wholeSet$sysbench
ycruncher <- wholeSet$ycruncher
iperf <- wholeSet$iperf * 1000
sds <- data.frame(vector(), vector(), vector(), vector())
benchmarks <- c("pgbench", "sysbench", "ycruncher", "iperf")
colnames(sds) <- benchmarks
head(sds)
for (i in 1:48) {
set <- wholeSet[wholeSet$setId == i, ]
a <- sd(set$pgbench)
b <- sd(set$sysbench)
c <- sd(set$ycruncher)
d <- sd(set$iperf)
sds <- rbind(sds, cbind(a, b, c, d))
}
sd(wholeSet$pgbench[wholeSet$setId == 1])
sd(wholeSet$pgbench[wholeSet$setId == 2])
sd(wholeSet$sysbench[wholeSet$setId == 1])
sd(wholeSet$sysbench[wholeSet$setId == 2])
sd(wholeSet$ycruncher[wholeSet$setId == 1])
sd(wholeSet$ycruncher[wholeSet$setId == 2])
sd(wholeSet$iperf[wholeSet$setId == 1])
sd(wholeSet$iperf[wholeSet$setId == 2])
View(wholeSet)
